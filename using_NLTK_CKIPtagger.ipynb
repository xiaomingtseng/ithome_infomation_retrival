{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h3>此處是使用NLTK處理英文句子的範例</h3>"
      ],
      "metadata": {
        "id": "oTBSrPF0qWbV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjWtNuCU9qBO",
        "outputId": "b869375b-a71f-4cb9-9db1-7fa14fe55ec7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TF_ngx3J9uYM",
        "outputId": "0404ebc4-da18-423a-e793-3dddde88065a"
      },
      "source": [
        "sent = \"“They asked me ‘why?’. And they told me ‘never again.’  A 59-year-old man is being held.\"\n",
        "sent = sent.replace('\\n', ' ')\n",
        "sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'“They asked me ‘why?’. And they told me ‘never again.’  A 59-year-old man is being held.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7viZGgkU9ubV",
        "outputId": "3c70806b-7909-4d17-ce24-29887cc2ccb6"
      },
      "source": [
        "word_set = nltk.word_tokenize(sent)\n",
        "word_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['“',\n",
              " 'They',\n",
              " 'asked',\n",
              " 'me',\n",
              " '‘',\n",
              " 'why',\n",
              " '?',\n",
              " '’',\n",
              " '.',\n",
              " 'And',\n",
              " 'they',\n",
              " 'told',\n",
              " 'me',\n",
              " '‘',\n",
              " 'never',\n",
              " 'again.',\n",
              " '’',\n",
              " 'A',\n",
              " '59-year-old',\n",
              " 'man',\n",
              " 'is',\n",
              " 'being',\n",
              " 'held',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = [\"Chile's\",\n",
        "\"Mr. O'Neill\",\n",
        "\"co-education\",\n",
        "\"the hold-him-back-and-drag-him-away maneuver\",\n",
        "\"data base\",\n",
        "\"San Francisco\",\n",
        "\"Los Angeles-based company\",\n",
        "\"cheap San Francisco-Los Angeles fares\",\n",
        "\"York University vs. New York University\",\n",
        "\"3/20/91\",\n",
        "\"20/3/91\",\n",
        "\"Mar 20, 1991\",\n",
        "\"B-52\",\n",
        "\"100.2.86.144\",\n",
        "\"(800) 234-2333\",\n",
        "\"800.234.2333\"\n",
        "]\n",
        "for txt in test_text:\n",
        "  t_list = nltk.word_tokenize(txt)\n",
        "  print(txt + \" ===> \" + ' '.join(t_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNwep6I7z2JN",
        "outputId": "0ab99a45-ed8f-4511-d0e1-9216fbce8ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chile's ===> Chile 's\n",
            "Mr. O'Neill ===> Mr. O'Neill\n",
            "co-education ===> co-education\n",
            "the hold-him-back-and-drag-him-away maneuver ===> the hold-him-back-and-drag-him-away maneuver\n",
            "data base ===> data base\n",
            "San Francisco ===> San Francisco\n",
            "Los Angeles-based company ===> Los Angeles-based company\n",
            "cheap San Francisco-Los Angeles fares ===> cheap San Francisco-Los Angeles fares\n",
            "York University vs. New York University ===> York University vs. New York University\n",
            "3/20/91 ===> 3/20/91\n",
            "20/3/91 ===> 20/3/91\n",
            "Mar 20, 1991 ===> Mar 20 , 1991\n",
            "B-52 ===> B-52\n",
            "100.2.86.144 ===> 100.2.86.144\n",
            "(800) 234-2333 ===> ( 800 ) 234-2333\n",
            "800.234.2333 ===> 800.234.2333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8DekwV29uec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ceab6e3-a396-414a-f496-dbe3c9c199f5"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv62O-GkYAQI",
        "outputId": "6d506636-34d4-44a3-a3b3-479d0027192a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kebuJW9uk8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "354513dc-c9de-4aad-c7fc-5bc41501164a"
      },
      "source": [
        "lemma_set = []\n",
        "for w in word_set:\n",
        "  lemma = lemmatizer.lemmatize(w, 'v')\n",
        "  lemma_set.append(lemma)\n",
        "new_sent = ' '.join(lemma_set)\n",
        "new_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'“ They ask me ‘ why ? ’ . And they tell me ‘ never again. ’ A 59-year-old man be be hold .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuYzOx8-9rdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd28763-dded-4902-8844-14d7d0993804"
      },
      "source": [
        "word_set = [\n",
        "  {'word':'mice', 'pos':'n'},\n",
        "  {'word':'went', 'pos':'v'},\n",
        "  {'word':'Chinese', 'pos':'n'},\n",
        "  {'word':'cutting', 'pos':'v'},\n",
        "  {'word':'happier', 'pos':'a'}]\n",
        "for w in word_set:\n",
        "  lemma = lemmatizer.lemmatize(w['word'], w['pos'])\n",
        "  print(w['word'] + ' ==> ' + lemma)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mice ==> mouse\n",
            "went ==> go\n",
            "Chinese ==> Chinese\n",
            "cutting ==> cut\n",
            "happier ==> happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import *"
      ],
      "metadata": {
        "id": "GfClG3VCz8bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_stemmer = PorterStemmer()\n",
        "sb_stemmer = SnowballStemmer(\"english\")\n",
        "lc_stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "7IERQ83KYqaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
        "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
        "            'meeting', 'stating', 'siezing', 'itemization',\n",
        "            'sensational', 'traditional', 'reference', 'colonizer',\n",
        "            'plotted']"
      ],
      "metadata": {
        "id": "BQDx-qKNY11s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in plurals:\n",
        "  s = pt_stemmer.stem(w)\n",
        "  x = sb_stemmer.stem(w)\n",
        "  y = lc_stemmer.stem(w)\n",
        "  print(w, ' ==> ', s, ',', x, ',', y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KggR2DHsY9c6",
        "outputId": "229722c4-b25e-4a26-d00b-4536525c8943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caresses  ==>  caress , caress , caress\n",
            "flies  ==>  fli , fli , fli\n",
            "dies  ==>  die , die , die\n",
            "mules  ==>  mule , mule , mul\n",
            "denied  ==>  deni , deni , deny\n",
            "died  ==>  die , die , died\n",
            "agreed  ==>  agre , agre , agree\n",
            "owned  ==>  own , own , own\n",
            "humbled  ==>  humbl , humbl , humbl\n",
            "sized  ==>  size , size , siz\n",
            "meeting  ==>  meet , meet , meet\n",
            "stating  ==>  state , state , stat\n",
            "siezing  ==>  siez , siez , siez\n",
            "itemization  ==>  item , item , item\n",
            "sensational  ==>  sensat , sensat , sens\n",
            "traditional  ==>  tradit , tradit , tradit\n",
            "reference  ==>  refer , refer , ref\n",
            "colonizer  ==>  colon , colon , colon\n",
            "plotted  ==>  plot , plot , plot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>此處是使用處理中文句子斷詞的範例，包括CKIPtagger及Distiltag</h3>"
      ],
      "metadata": {
        "id": "7vVfZfoxqu6-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AuqXp259rhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb535205-31b2-42a7-b571-ae98419a1a3b"
      },
      "source": [
        "pip install -U DistilTag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting DistilTag\n",
            "  Downloading DistilTag-0.2.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (from DistilTag) (4.6.6)\n",
            "Collecting transformers>=3.2\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.9/dist-packages (from DistilTag) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from DistilTag) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->DistilTag) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->DistilTag) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->DistilTag) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->DistilTag) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->DistilTag) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->DistilTag) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->DistilTag) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->DistilTag) (3.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.2->DistilTag) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.2->DistilTag) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.2->DistilTag) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers>=3.2->DistilTag) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.2->DistilTag) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown->DistilTag) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown->DistilTag) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown->DistilTag) (2.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6->DistilTag) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=3.2->DistilTag) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=3.2->DistilTag) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=3.2->DistilTag) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=3.2->DistilTag) (1.26.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests->transformers>=3.2->DistilTag) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6->DistilTag) (1.3.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, DistilTag\n",
            "Successfully installed DistilTag-0.2.2 huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvpYnakT9ro0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cde2a8d-9463-4e4b-e3ef-d8876d7e15d0"
      },
      "source": [
        "import DistilTag\n",
        "DistilTag.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AzUICPQ5MMt_IWg4JZ3mWM6vGbQkv01L\n",
            "To: /tmp/tmp_apnfjkddistiltag/tagmodel.zip\n",
            "100%|██████████| 501M/501M [00:10<00:00, 49.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting up model...\n",
            "DistilTag model installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUIHi3IW9rr0"
      },
      "source": [
        "from DistilTag import DistilTag\n",
        "tagger = DistilTag()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"綜合生魚片中的旗魚非常乾，有點難以下嚥，鮪魚有一點腥，其餘皆尚可，炒松阪肉味道調很好，肉質Q彈有嚼勁，蝦子給的算蠻豐盛蠻大隻的，但吃起來沒有很甜，肉有點老，炸白身魚外酥內軟，還有炸米餅，口感層次很多，很推一的一道菜\"\n",
        "output = tagger.tag(sentence)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3Jin4V0r9QW",
        "outputId": "6d580f88-7e1b-41b7-e361-4016d4e6f125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('綜合', 'A'),\n",
              "  ('生魚片', 'Na'),\n",
              "  ('中', 'Ng'),\n",
              "  ('的', 'DE'),\n",
              "  ('旗魚', 'Na'),\n",
              "  ('非常', 'Dfa'),\n",
              "  ('乾', 'VH'),\n",
              "  ('，', 'COMMACATEGORY')],\n",
              " [('有點', 'Dfa'), ('難以', 'D'), ('下嚥', 'VA'), ('，', 'COMMACATEGORY')],\n",
              " [('鮪魚', 'Na'),\n",
              "  ('有', 'V_2'),\n",
              "  ('一點', 'Neqa'),\n",
              "  ('腥', 'Na'),\n",
              "  ('，', 'COMMACATEGORY')],\n",
              " [('其餘', 'Neqa'), ('皆', 'D'), ('尚可', 'VH'), ('，', 'COMMACATEGORY')],\n",
              " [('炒', 'VC'),\n",
              "  ('松阪', 'Nc'),\n",
              "  ('肉味道', 'Na'),\n",
              "  ('調', 'Na'),\n",
              "  ('很', 'Dfa'),\n",
              "  ('好', 'VH'),\n",
              "  ('，', 'COMMACATEGORY')],\n",
              " [('肉質', 'Na'),\n",
              "  ('Ｑ彈', 'Na'),\n",
              "  ('有', 'V_2'),\n",
              "  ('嚼勁', 'Na'),\n",
              "  ('，', 'COMMACATEGORY')],\n",
              " [('蝦子', 'Na'),\n",
              "  ('給', 'VD'),\n",
              "  ('的', 'DE'),\n",
              "  ('算', 'VG'),\n",
              "  ('蠻', 'Dfa'),\n",
              "  ('豐盛', 'VH'),\n",
              "  ('蠻', 'Dfa'),\n",
              "  ('大', 'VH'),\n",
              "  ('隻', 'Nf'),\n",
              "  ('的', 'DE'),\n",
              "  ('，', 'COMMACATEGORY')],\n",
              " [('但', 'Cbb'),\n",
              "  ('吃起來', 'D'),\n",
              "  ('沒有', 'D'),\n",
              "  ('很', 'Dfa'),\n",
              "  ('甜', 'VH'),\n",
              "  ('，', 'COMMACATEGORY')],\n",
              " [('肉', 'Na'), ('有點', 'Dfa'), ('老', 'VH'), ('，', 'COMMACATEGORY')],\n",
              " [('炸', 'VC'),\n",
              "  ('白', 'Na'),\n",
              "  ('身魚', 'Na'),\n",
              "  ('外', 'Ncd'),\n",
              "  ('酥', 'Na'),\n",
              "  ('內', 'Ncd'),\n",
              "  ('軟', 'VH'),\n",
              "  ('，', 'COMMACATEGORY')],\n",
              " [('還', 'D'), ('有', 'V_2'), ('炸米餅', 'Na'), ('，', 'COMMACATEGORY')],\n",
              " [('口感', 'Na'), ('層次', 'Na'), ('很多', 'VH'), ('，', 'COMMACATEGORY')],\n",
              " [('很', 'Dfa'),\n",
              "  ('推', 'VC'),\n",
              "  ('一', 'Neu'),\n",
              "  ('的', 'DE'),\n",
              "  ('一', 'Neu'),\n",
              "  ('道', 'Nf'),\n",
              "  ('菜', 'Na')]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_y__vUpr9TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikdo30qW9rvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ca2c7d-a1a5-446c-a302-c75e284ae91b"
      },
      "source": [
        "pip install -U ckiptagger[tf,gdown]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ckiptagger[gdown,tf]\n",
            "  Downloading ckiptagger-0.2.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.9/dist-packages (from ckiptagger[gdown,tf]) (2.12.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (from ckiptagger[gdown,tf]) (4.6.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.12.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (67.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.2.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.22.4)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.14.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (23.3.3)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.3.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.4.7)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.32.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.20.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (16.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown->ckiptagger[gdown,tf]) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown->ckiptagger[gdown,tf]) (3.11.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown->ckiptagger[gdown,tf]) (4.11.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown->ckiptagger[gdown,tf]) (2.27.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.0.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.10.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.17.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown->ckiptagger[gdown,tf]) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown->ckiptagger[gdown,tf]) (1.7.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (6.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->ckiptagger[gdown,tf]) (3.2.2)\n",
            "Installing collected packages: ckiptagger\n",
            "Successfully installed ckiptagger-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ckiptagger import data_utils, construct_dictionary, WS, POS\n",
        "data_utils.download_data_gdown(\"./\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHS8fZAYVKh5",
        "outputId": "818e450c-0430-4fa2-c7d1-7ca0e62d4f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
            "To: /content/data.zip\n",
            "100%|██████████| 1.88G/1.88G [00:23<00:00, 81.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ws = WS(\"./data\")\n",
        "pos = POS(\"./data\")"
      ],
      "metadata": {
        "id": "S3JFsiu4VbNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b86c086-e4f8-41be-a8ec-e581a58ca18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDTcqgmo9ryM"
      },
      "source": [
        "sentence_list = [\n",
        "    \"綜合生魚片中的旗魚非常乾，有點難以下嚥，鮪魚有一點腥，其餘皆尚可，炒松阪肉味道調很好，肉質Q彈有嚼勁，蝦子給的算蠻豐盛蠻大隻的，但吃起來沒有很甜，肉有點老，炸白身魚外酥內軟，還有炸米餅，口感層次很多，很推一的一道菜\",\n",
        "    \"點了皇家牛來的時候外觀真的是下一位。肥油沒處理筋一堆真的是不知道在吃什麼很反胃\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sentence_list = ws(sentence_list)"
      ],
      "metadata": {
        "id": "Kn2lmRp0vrDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sentence_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxRNlu8yvrGA",
        "outputId": "e09e4b68-c5b1-4150-c507-3c5b4b8e1887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['綜合',\n",
              "  '生魚片',\n",
              "  '中',\n",
              "  '的',\n",
              "  '旗魚',\n",
              "  '非常',\n",
              "  '乾',\n",
              "  '，',\n",
              "  '有點',\n",
              "  '難以',\n",
              "  '下嚥',\n",
              "  '，',\n",
              "  '鮪魚',\n",
              "  '有',\n",
              "  '一點',\n",
              "  '腥',\n",
              "  '，',\n",
              "  '其餘',\n",
              "  '皆',\n",
              "  '尚可',\n",
              "  '，',\n",
              "  '炒',\n",
              "  '松阪肉',\n",
              "  '味道',\n",
              "  '調',\n",
              "  '很',\n",
              "  '好',\n",
              "  '，',\n",
              "  '肉質',\n",
              "  'Q彈',\n",
              "  '有',\n",
              "  '嚼勁',\n",
              "  '，',\n",
              "  '蝦子',\n",
              "  '給',\n",
              "  '的',\n",
              "  '算',\n",
              "  '蠻',\n",
              "  '豐盛',\n",
              "  '蠻',\n",
              "  '大',\n",
              "  '隻',\n",
              "  '的',\n",
              "  '，',\n",
              "  '但',\n",
              "  '吃起來',\n",
              "  '沒有',\n",
              "  '很',\n",
              "  '甜',\n",
              "  '，',\n",
              "  '肉',\n",
              "  '有點',\n",
              "  '老',\n",
              "  '，',\n",
              "  '炸',\n",
              "  '白身魚',\n",
              "  '外酥',\n",
              "  '內',\n",
              "  '軟',\n",
              "  '，',\n",
              "  '還',\n",
              "  '有',\n",
              "  '炸米餅',\n",
              "  '，',\n",
              "  '口感',\n",
              "  '層次',\n",
              "  '很多',\n",
              "  '，',\n",
              "  '很',\n",
              "  '推',\n",
              "  '一',\n",
              "  '的',\n",
              "  '一',\n",
              "  '道',\n",
              "  '菜'],\n",
              " ['點',\n",
              "  '了',\n",
              "  '皇家',\n",
              "  '牛',\n",
              "  '來',\n",
              "  '的',\n",
              "  '時候',\n",
              "  '外觀',\n",
              "  '真的',\n",
              "  '是',\n",
              "  '下',\n",
              "  '一',\n",
              "  '位',\n",
              "  '。',\n",
              "  '肥油',\n",
              "  '沒',\n",
              "  '處理',\n",
              "  '筋',\n",
              "  '一',\n",
              "  '堆',\n",
              "  '真的',\n",
              "  '是',\n",
              "  '不',\n",
              "  '知道',\n",
              "  '在',\n",
              "  '吃',\n",
              "  '什麼',\n",
              "  '很',\n",
              "  '反胃']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_sentence_list = pos(word_sentence_list)\n",
        "pos_sentence_list"
      ],
      "metadata": {
        "id": "DHrAj8UOwGEm",
        "outputId": "be4a7f2c-6dcb-4ba6-add1-594ee925c19f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A',\n",
              "  'Na',\n",
              "  'Ng',\n",
              "  'DE',\n",
              "  'Na',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Dfa',\n",
              "  'D',\n",
              "  'VA',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'V_2',\n",
              "  'Neqa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Neqa',\n",
              "  'D',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'VC',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'V_2',\n",
              "  'Na',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'VD',\n",
              "  'DE',\n",
              "  'VG',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'Nf',\n",
              "  'T',\n",
              "  'COMMACATEGORY',\n",
              "  'Cbb',\n",
              "  'D',\n",
              "  'D',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'Dfa',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'VC',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'Ncd',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'D',\n",
              "  'V_2',\n",
              "  'Na',\n",
              "  'COMMACATEGORY',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'VH',\n",
              "  'COMMACATEGORY',\n",
              "  'Dfa',\n",
              "  'VC',\n",
              "  'Neu',\n",
              "  'DE',\n",
              "  'Neu',\n",
              "  'Nf',\n",
              "  'Na'],\n",
              " ['VC',\n",
              "  'Di',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'VA',\n",
              "  'DE',\n",
              "  'Na',\n",
              "  'Na',\n",
              "  'D',\n",
              "  'SHI',\n",
              "  'Nes',\n",
              "  'Neu',\n",
              "  'Nf',\n",
              "  'PERIODCATEGORY',\n",
              "  'Na',\n",
              "  'D',\n",
              "  'VC',\n",
              "  'Na',\n",
              "  'Neu',\n",
              "  'Nf',\n",
              "  'D',\n",
              "  'SHI',\n",
              "  'D',\n",
              "  'VK',\n",
              "  'D',\n",
              "  'VC',\n",
              "  'Nep',\n",
              "  'Dfa',\n",
              "  'VH']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRoErrZVwGMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCtA99OQydVv"
      },
      "source": [
        "# https://www.nltk.org/\n",
        "import nltk\n",
        "import json\n",
        "from google.colab import files # 在 Google Colab 環境上傳檔案時所用；若在個人電腦執行 Python 則不需要\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afVpBzzya7Uu"
      },
      "source": [
        "# 若下載 python 檔並在自己電腦執行，開檔讀檔請用這段 # 若要上傳檔案到 Google Colab 虛擬主機，請註解掉這段\n",
        "#f = open(\"D:/cjlin/sample/ReutersCorn-sample10.json\", \"r\", encoding='UTF-8') # encoding='UTF-8' 才能正確讀入中文檔案\n",
        "#docText = f.read()\n",
        "#f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szej4JgszM_i"
      },
      "source": [
        "# 由電腦上傳檔案至 Colab 虛擬主機 # 若要在個人電腦單機作業，請註解掉這段\n",
        "file = files.upload()\n",
        "trf = open(\"ReutersCorn-train.json\", \"r\", encoding='UTF-8')\n",
        "fdoc = trf.read()\n",
        "trf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfG1FHwyI1aV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYkRVZhDI11E"
      },
      "source": [
        "dataset = json.loads(fdoc)\n",
        "dataset[0]['text'] # 此行僅用來確認執行結果正確，可刪去"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHjR_iQI66q"
      },
      "source": [
        "file = files.upload()\n",
        "dictf = open(\"posDict_ReutersCorn.json\", \"r\", encoding='UTF-8')\n",
        "dict_raw = dictf.read()\n",
        "dictf.close()\n",
        "POSdict = json.loads(dict_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FccRYuKpJdj0"
      },
      "source": [
        "wordTotal = 0\n",
        "tokenSet = set()\n",
        "for doc in dataset:\n",
        "  word = nltk.word_tokenize(doc['text'])\n",
        "  wordTotal += len(word)\n",
        "  #nltk_token = []\n",
        "  for w in word:\n",
        "    tokenSet.add(w)\n",
        "    #nltk_token.append(w)\n",
        "  #doc['nltk_token'] = nltk_token\n",
        "print(wordTotal)\n",
        "len(tokenSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBCTag6SYiY"
      },
      "source": [
        "lowerSet = set()\n",
        "for w in tokenSet:\n",
        "  lowerSet.add(w.lower())\n",
        "len(lowerSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-0kTVS0fOxl"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "d7wfjrc_VqZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sets = ['automate', 'automatic', 'automation', 'computer', 'computerize', 'computerization', 'computerizational']\n",
        "for w in word_sets:\n",
        "  stemstr = lemmatizer.lemmatize(w, \"v\")\n",
        "  print(w + ' ==> ' + stemstr)"
      ],
      "metadata": {
        "id": "P5fma8I6Vao6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acqHuXPWVay_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zh1sYiTkVa2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sus2mFI2eIAd"
      },
      "source": [
        "lemmaWOpos = set()\n",
        "for w in lowerSet:\n",
        "  lemmaWOpos.add(lemmatizer.lemmatize(w))\n",
        "len(lemmaWOpos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZdzoz17Lh0B"
      },
      "source": [
        "lemmaWpos = set()\n",
        "for w in lowerSet:\n",
        "  pos = POSdict.get(w)\n",
        "  if pos != None and pos != \"x\":\n",
        "    if pos == \"j\": pos = \"a\"\n",
        "    lemmaWpos.add(lemmatizer.lemmatize(w, pos))\n",
        "  else:\n",
        "    lemmaWpos.add(lemmatizer.lemmatize(w))\n",
        "len(lemmaWpos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWkPiqbif10E"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BMXL0Kfcxc"
      },
      "source": [
        "stemSet = set()\n",
        "for w in lowerSet:\n",
        "  stemSet.add(stemmer.stem(w))\n",
        "len(stemSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "pxF9xIDYY7Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_sets = ['automate', 'automatic', 'automation', 'computer', 'computerize', 'computerization', 'computerizational']\n",
        "for w in word_sets:\n",
        "  stemstr = stemmer.stem(w)\n",
        "  print(w + ' ==> ' + stemstr)\n"
      ],
      "metadata": {
        "id": "F1QesJ85z_te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U ckiptagger[tf,gdown]"
      ],
      "metadata": {
        "id": "FD3Lb0o0apWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER"
      ],
      "metadata": {
        "id": "kYsB82y-awlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_utils.download_data_gdown(\"./\")"
      ],
      "metadata": {
        "id": "ytrXrn2za6oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ws = WS(\"./data\")\n",
        "pos = POS(\"./data\")"
      ],
      "metadata": {
        "id": "J3Qv5BSxbGqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = [\n",
        "    \"土地公有政策?？還是土地婆有政策。.\",\n",
        "    \"某某候選人提出的土地公有政策\",\n",
        "    \"最多容納59,000個人,或5.9萬人,再多就不行了.這是環評的結論.\",\n",
        "    \"電子計算機是會計算題目的機器。\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "gQKcF1JFbToo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_seg = ws([\"高高興興打打球,丟丟看成績如何、開不開心\"])"
      ],
      "metadata": {
        "id": "GFJ6wM8idf_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_seg[0]"
      ],
      "metadata": {
        "id": "myknoTXzdmPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_pos = pos(sent_seg)\n",
        "sent_pos[0]"
      ],
      "metadata": {
        "id": "Iir58IfOmuJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}